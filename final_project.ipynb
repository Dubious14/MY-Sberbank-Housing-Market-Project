{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMBcUQaSzbdxt9BmHZoQBU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dubious14/MY-Sberbank-Housing-Market-Project/blob/main/final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "h677plD2ezIc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import scipy.stats as stats"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Please upload the train file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "file_name = list(uploaded.keys())[0]\n",
        "\n",
        "try:\n",
        "    train_original = pd.read_csv(file_name)\n",
        "    print(f\"\\nFile '{file_name}' uploaded successfully!\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Error reading the file: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Gu46t5vvUTz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Please upload the test file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "file_name = list(uploaded.keys())[0]\n",
        "\n",
        "try:\n",
        "    test_original = pd.read_csv(file_name)\n",
        "    print(f\"\\nFile '{file_name}' uploaded successfully!\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Error reading the file: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "whrKi3dJ_ZyI",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train_original.copy()\n",
        "test = test_original.copy()\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EH2nGg-gUWDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "train.describe(include='all')\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9kC0RdN-iZaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply log transformation to `price_doc`\n",
        "\n",
        "train['log_price_doc'] = np.log1p(train['price_doc'])"
      ],
      "metadata": {
        "id": "cZVAdXMOynWQ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train_original.copy()\n",
        "test = test_original.copy()\n",
        "train['log_price_doc'] = np.log1p(train['price_doc'])"
      ],
      "metadata": {
        "id": "GGcKs_uAMlxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_replace = [\"full_sq\", \"life_sq\", \"material\", \"num_room\", \"kitch_sq\", \"max_floor\"]\n",
        "train[columns_to_replace] = train[columns_to_replace].replace(0, np.nan)\n",
        "test[columns_to_replace] = test[columns_to_replace].replace(0, np.nan)\n",
        "\n",
        "\n",
        "replace = {117:17, 99:np.nan}\n",
        "train['max_floor'].replace(to_replace = replace, inplace = True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "z0wZrTc0rUdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cheking built_year\n",
        "\n",
        "# print(train[\"build_year\"].dropna().sort_values().head(50))\n",
        "\n",
        "replace = {215:2015, 0:np.nan, 1:np.nan, 3:np.nan, 20:np.nan, 71:np.nan}\n",
        "train['build_year'].replace(to_replace = replace, inplace = True)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wJvpNWv5w52e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train[\"build_year\"].dropna().sort_values(ascending=False).head(50))\n",
        "# saw an extreme value in the describe\n",
        "train.loc[train[\"build_year\"] == 20052009, \"build_year\"] = 2005\n",
        "train.loc[train[\"build_year\"] == 4965, \"build_year\"] = 1965\n",
        "\n",
        "# then saw 2018 which led us think that there were transaction that were taken before the house building was completed so we created a binary feature\n",
        "# train.loc[train[\"build_year\"].notna(), \"pre_sale\"] = (train[\"build_year\"] > train[\"timestamp\"].dt.year).astype(int)\n",
        "# we will create it only after treating na values in build_year\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WEaz3sUlrdbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Original Distribution\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(train['price_doc'], kde=True, bins=30, color='skyblue')\n",
        "plt.title('Original Distribution of price_doc', fontsize=16)\n",
        "plt.xlabel('price_doc', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Log-Transformed Distribution\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(train['log_price_doc'], kde=True, bins=30, color='orange')\n",
        "plt.title('Log-Transformed Distribution of price_doc', fontsize=16)\n",
        "plt.xlabel('log(price_doc)', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# train.drop(columns=['price_doc'], inplace=True)\n",
        "# train.rename(columns={'log_price_doc': 'price_doc'}, inplace=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "aznYbs-GUxgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "# Display the column names and their data types\n",
        "column_types = train.dtypes.reset_index()\n",
        "column_types.columns = ['Column', 'Data Type']\n",
        "\n",
        "# Print the result\n",
        "print(column_types)\n",
        "\n",
        "# Reset display option if needed\n",
        "pd.reset_option('display.max_rows')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "T655jT9MUyUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "object_columns = column_types[(column_types['Data Type'] == 'object') & (column_types['Column'] != 'sub_area')]['Column']\n",
        "\n",
        "# Plot categorical columns with count plots & box plots\n",
        "for col in object_columns:\n",
        "    plt.figure(figsize=(16, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.countplot(data=train, x=col, palette='viridis', hue=col)\n",
        "    plt.title(f'Distribution of {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.boxplot(data=train, x=col, y=\"log_price_doc\", palette='viridis', hue=col)\n",
        "    plt.title(f'Box Plot of log_price_doc for {col}')\n",
        "    plt.ylabel('log_price_doc')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cUIn54e9XAKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16, 8))\n",
        "\n",
        "sns.boxplot(data=train, x='product_type', y='log_price_doc', palette='muted')\n",
        "plt.title('Box Plot of House Prices by Product Type')\n",
        "plt.xlabel('Product Type')\n",
        "plt.ylabel('House Price (price_doc)')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RcXsJ4_tX6OU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Extract log-transformed house prices for each category\n",
        "investment_prices = train[train['product_type'] == 'Investment']['log_price_doc']\n",
        "owner_prices = train[train['product_type'] == 'OwnerOccupier']['log_price_doc']\n",
        "\n",
        "# Plot both distributions on the same grid\n",
        "sns.kdeplot(investment_prices, fill=True, color='#f32e04', alpha=0.8, label='Investment')\n",
        "sns.kdeplot(owner_prices, fill=True, color='skyblue', alpha=0.8, label='OwnerOccupier')\n",
        "\n",
        "# Customize plot\n",
        "plt.title('Density Plot of House Prices: Investment vs Owner-Occupier')\n",
        "plt.xlabel('Log House Price (log_price_doc)')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VBEi78rjaVJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# conducting t test of means to check if Investment housing price is statistically significant from OwnerOccupier\n",
        "# if it is, we might need to treat them diffrently when predicting housing price\n",
        "\n",
        "random = 14\n",
        "\n",
        "# Randomly sample 5000 observations from each group\n",
        "investment_prices = train[train['product_type'] == 'Investment']['log_price_doc'].sample(n=5000, replace=False, random_state=random)\n",
        "owner_prices = train[train['product_type'] == 'OwnerOccupier']['log_price_doc'].sample(n=5000, replace=False, random_state=random)\n",
        "\n",
        "# Print sample sizes to verify\n",
        "print(f\"Sample size - Investment: {len(investment_prices)}, OwnerOccupier: {len(owner_prices)}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3CpI-cdKdA2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shapiro-Wilk Test for normality (H0: Data is normally distributed)\n",
        "shapiro_investment = stats.shapiro(investment_prices)\n",
        "shapiro_owner = stats.shapiro(owner_prices)\n",
        "\n",
        "print(f\"Shapiro-Wilk Test (Investment): p-value = {shapiro_investment.pvalue:.5f}\")\n",
        "print(f\"Shapiro-Wilk Test (OwnerOccupier): p-value = {shapiro_owner.pvalue:.5f}\")\n",
        "\n",
        "# Histogram and Q-Q plots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "sns.histplot(investment_prices, kde=True, bins=30, ax=axes[0], color=\"blue\", alpha=0.6)\n",
        "axes[0].set_title(\"Investment Price Distribution\")\n",
        "\n",
        "sns.histplot(owner_prices, kde=True, bins=30, ax=axes[1], color=\"green\", alpha=0.6)\n",
        "axes[1].set_title(\"OwnerOccupier Price Distribution\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# QQ-Plots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "stats.probplot(investment_prices, dist=\"norm\", plot=axes[0])\n",
        "axes[0].set_title(\"Q-Q Plot: Investment\")\n",
        "\n",
        "stats.probplot(owner_prices, dist=\"norm\", plot=axes[1])\n",
        "axes[1].set_title(\"Q-Q Plot: OwnerOccupier\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"Investment Prices Data Type:\", investment_prices.dtype)\n",
        "print(\"OwnerOccupier Prices Data Type:\", owner_prices.dtype)\n",
        "\n",
        "# Levene’s Test (H0: Variances are equal)\n",
        "levene_test = stats.levene(investment_prices, owner_prices)\n",
        "print(f\"Levene's Test for Equal Variance: p-value = {levene_test.pvalue:.5f}\")\n",
        "\n",
        "# investment showed weird distribution in its lower values in the QQ plot, we further analyze it to see why it happens and how can we handle this"
      ],
      "metadata": {
        "collapsed": true,
        "id": "lGE4bz2LcPFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Welch’s t-test (since variances are not equal)\n",
        "welch_ttest = stats.ttest_ind(investment_prices, owner_prices, equal_var=False)\n",
        "\n",
        "# Print the result\n",
        "print(f\"Welch’s T-Test: t-value = {welch_ttest.statistic:.5f}, p-value = {welch_ttest.pvalue:.5f}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BBsSrooPdujP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# columns we suspect that don't change based on the region(sub_arae), so we can replace na values with the mode(which is for most features - the only unique value)\n",
        "columns_to_analyze = [\n",
        "    \"area_m\", \"raion_popul\", \"green_zone_part\", \"indust_part\", \"children_preschool\",\n",
        "    \"preschool_quota\", \"preschool_education_centers_raion\", \"children_school\", \"school_quota\",\n",
        "    \"school_education_centers_raion\", \"school_education_centers_top_20_raion\", \"hospital_beds_raion\",\n",
        "    \"healthcare_centers_raion\", \"university_top_20_raion\", \"sport_objects_raion\",\n",
        "    \"additional_education_raion\", \"culture_objects_top_25\", \"culture_objects_top_25_raion\",\n",
        "    \"shopping_centers_raion\", \"office_raion\", \"thermal_power_plant_raion\", \"incineration_raion\",\n",
        "    \"oil_chemistry_raion\", \"radiation_raion\", \"railroad_terminal_raion\", \"big_market_raion\",\n",
        "    \"nuclear_reactor_raion\", \"detention_facility_raion\", \"full_all\", \"male_f\", \"female_f\",\n",
        "    \"young_all\", \"young_male\", \"young_female\", \"work_all\", \"work_male\", \"work_female\",\n",
        "    \"ekder_all\", \"ekder_male\", \"ekder_female\", \"0_6_all\", \"0_6_male\", \"0_6_female\",\n",
        "    \"7_14_all\", \"7_14_male\", \"7_14_female\", \"0_17_all\", \"0_17_male\", \"0_17_female\",\n",
        "    \"16_29_all\", \"16_29_male\", \"16_29_female\", \"0_13_all\", \"0_13_male\", \"0_13_female\",\n",
        "    \"raion_build_count_with_material_info\", \"build_count_block\", \"build_count_wood\",\n",
        "    \"build_count_frame\", \"build_count_brick\", \"build_count_monolith\", \"build_count_panel\",\n",
        "    \"build_count_foam\", \"build_count_slag\", \"build_count_mix\", \"raion_build_count_with_builddate_info\",\n",
        "    \"build_count_before_1920\", \"build_count_1921-1945\", \"build_count_1946-1970\",\n",
        "    \"build_count_1971-1995\", \"build_count_after_1995\", \"ID_metro\"\n",
        "]"
      ],
      "metadata": {
        "id": "kyPSp2_Zr956"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify numerical and categorical columns\n",
        "numerical_cols = [col for col in columns_to_analyze if train[col].dtype in ['int64', 'float64']]\n",
        "categorical_cols = [col for col in columns_to_analyze if train[col].dtype == 'object']\n",
        "\n",
        "# Perform analysis for numerical columns\n",
        "numerical_analysis = train.groupby(\"sub_area\")[numerical_cols].agg(\n",
        "    [\"median\", \"mean\", lambda x: x.mode().iloc[0] if not x.mode().empty else None, \"nunique\"]\n",
        ")\n",
        "\n",
        "# Rename the lambda function column for clarity\n",
        "numerical_analysis.rename(columns={\"<lambda_0>\": \"mode\"}, inplace=True)\n",
        "\n",
        "# Perform analysis for categorical columns\n",
        "categorical_analysis = train.groupby(\"sub_area\")[categorical_cols].agg(\n",
        "    [lambda x: x.mode().iloc[0] if not x.mode().empty else None, \"nunique\"]\n",
        ")\n",
        "\n",
        "# Rename the lambda function column for clarity\n",
        "categorical_analysis.rename(columns={\"<lambda_0>\": \"mode\"}, inplace=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rM1Qs21Ap3sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "\n",
        "\n",
        "print(\"Numerical Features Analysis:\")\n",
        "display(numerical_analysis)\n",
        "print(\"\\nCategorical Features Analysis:\")\n",
        "display(categorical_analysis)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VG0bnOD-tJDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_filtered = numerical_analysis.xs(\"nunique\", axis=1, level=1).gt(1)\n",
        "numerical_filtered_features = numerical_filtered.apply(lambda x: list(x.index[x]), axis=1)\n",
        "\n",
        "# Filter categorical features where 'nunique' > 1 for each sub_area\n",
        "categorical_filtered = categorical_analysis.xs(\"nunique\", axis=1, level=1).gt(1)\n",
        "categorical_filtered_features = categorical_filtered.apply(lambda x: list(x.index[x]), axis=1)\n",
        "\n",
        "# Convert to DataFrames\n",
        "numerical_filtered_df = pd.DataFrame(numerical_filtered_features, columns=[\"Features with >1 Unique Values\"])\n",
        "categorical_filtered_df = pd.DataFrame(categorical_filtered_features, columns=[\"Features with >1 Unique Values\"])\n"
      ],
      "metadata": {
        "id": "0KPwfC0EuHWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "\n",
        "\n",
        "print(\"Numerical Features Analysis:\")\n",
        "display(numerical_filtered_df)\n",
        "print(\"\\nCategorical Features Analysis:\")\n",
        "display(categorical_filtered_df)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3HvhtjfbuSin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data filling\n",
        "\n",
        "# Identify numerical and categorical columns (excluding 'ID_metro')\n",
        "columns_to_analyze.remove(\"ID_metro\")\n",
        "# removed ID_metro becuase this is the only feature amongst columns_to_analyze that had more than 1 unique value (For most of the sub_areas)\n",
        "\n",
        "def fill_na_with_mode(group):\n",
        "    for col in columns_to_analyze:\n",
        "        mode_series = group[col].mode()\n",
        "        if not mode_series.empty:\n",
        "            group[col] = group[col].fillna(mode_series.iloc[0])\n",
        "        else:\n",
        "            # If no mode in the group, use the global mode of the column\n",
        "            global_mode = train[col].mode().iloc[0] if not train[col].mode().empty else None\n",
        "            if global_mode is not None:\n",
        "                group[col] = group[col].fillna(global_mode)\n",
        "    return group\n",
        "\n",
        "# Apply the function to each sub_area\n",
        "train = train.groupby(\"sub_area\", group_keys=False).apply(fill_na_with_mode)\n",
        "test = test.groupby(\"sub_area\", group_keys=False).apply(fill_na_with_mode)\n",
        "\n",
        "# Confirm that NA values are filled\n",
        "print(\"Missing values after imputation:\")\n",
        "print(train[columns_to_analyze].isna().sum())\n",
        "\n",
        "# Confirm that NA values are filled\n",
        "print(\"Missing values after imputation:\")\n",
        "print(test[columns_to_analyze].isna().sum())\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "arPN3SoxvwcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "missing_values = train.isna().sum()\n",
        "missing_percentage = (missing_values / len(train)) * 100\n",
        "\n",
        "na_summary = pd.DataFrame({\n",
        "    'Missing Values': missing_values,\n",
        "    'Percentage (%)': missing_percentage\n",
        "})\n",
        "\n",
        "# Filter columns with missing percentage > 0\n",
        "na_summary = na_summary[na_summary['Percentage (%)'] > 0].sort_values(by='Percentage (%)', ascending=True)\n",
        "\n",
        "# Display the summary\n",
        "print(\"Columns with Missing Values:\")\n",
        "print(na_summary)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ozwLpLB5VrqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot columns with missing percentage > 0\n",
        "plt.figure(figsize=(14, 7))\n",
        "sns.barplot(\n",
        "    x=na_summary.index,\n",
        "    y=na_summary['Percentage (%)'],\n",
        "    hue=na_summary.index,  # Set the `hue` to the same as `x` to utilize the palette\n",
        "    dodge=False,  # Disable hue-based offset\n",
        "    palette=\"coolwarm\"\n",
        ")\n",
        "plt.title('Percentage of Missing Values by Column (Only > 0%)', fontsize=16)\n",
        "plt.xlabel('Columns', fontsize=14)\n",
        "plt.ylabel('Percentage of Missing Values (%)', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend([], [], frameon=False)  # Disable the legend\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "K_ik--2OBzxk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filling NA for features with less than 30% NA\n",
        "\n",
        "na_summary = na_summary.reset_index()\n",
        "na_summary.columns = ['Column', 'Missing Values', 'Percentage (%)']\n",
        "\n",
        "filtered_na_summary = na_summary[na_summary['Percentage (%)'] < 30]\n",
        "\n",
        "# Split columns into two groups: those with \"avg\" in their name and those without\n",
        "avg_columns = filtered_na_summary[filtered_na_summary['Column'].str.contains(\"avg\", case=False)]['Column'].tolist()\n",
        "non_avg_columns = filtered_na_summary[~filtered_na_summary['Column'].str.contains(\"avg\", case=False)]['Column'].tolist()\n",
        "\n",
        "# we dont want to replace this with the median because we think they are very important and need to be carefully treated\n",
        "columns_to_exclude = ['floor', 'life_sq']\n",
        "for col in columns_to_exclude:\n",
        "  non_avg_columns.remove(col)\n",
        "\n",
        "\n",
        "# Print or save the results\n",
        "print(\"\\nColumns with 'avg' in their name (Less than 30% missing):\")\n",
        "print(avg_columns)\n",
        "\n",
        "print(\"\\nColumns without 'avg' in their name (Less than 30% missing):\")\n",
        "print(non_avg_columns)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mL4i_Xxp9Hgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Impute Train Based on Its Own Statistics\n",
        "for col in avg_columns:\n",
        "    train[col] = train.groupby('sub_area')[col].transform(lambda x: x.fillna(x.mean()))\n",
        "\n",
        "for col in non_avg_columns:\n",
        "    train[col] = train.groupby('sub_area')[col].transform(lambda x: x.fillna(x.median()))\n",
        "\n",
        "for col in avg_columns:\n",
        "    if col in train.columns:\n",
        "        train[col] = train[col].fillna(train[col].mean())\n",
        "\n",
        "for col in non_avg_columns:\n",
        "    if col in train.columns:\n",
        "        train[col] = train[col].fillna(train[col].median())\n",
        "\n",
        "# Step 2: Apply the Same Imputation to Test Using Train Statistics\n",
        "for col in avg_columns:\n",
        "    test[col] = test.groupby('sub_area')[col].transform(lambda x: x.fillna(train.groupby('sub_area')[col].transform('mean')))\n",
        "\n",
        "for col in non_avg_columns:\n",
        "    test[col] = test.groupby('sub_area')[col].transform(lambda x: x.fillna(train.groupby('sub_area')[col].transform('median')))\n",
        "\n",
        "for col in avg_columns:\n",
        "    if col in test.columns:\n",
        "        test[col] = test[col].fillna(train[col].mean())  # Use train mean for test\n",
        "\n",
        "for col in non_avg_columns:\n",
        "    if col in test.columns:\n",
        "        test[col] = test[col].fillna(train[col].median())  # Use train median for test\n",
        "\n",
        "# Step 3: Confirm Missing Values are Filled\n",
        "print(\"Missing values after imputation in Train:\")\n",
        "print(train[avg_columns + non_avg_columns].isna().sum())\n",
        "\n",
        "print(\"\\nMissing values after imputation in Test:\")\n",
        "print(test[avg_columns + non_avg_columns].isna().sum())\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "A60iaoP190DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#verify it worked\n",
        "\n",
        "missing_values = train.isna().sum()\n",
        "missing_percentager = (missing_values / len(train)) * 100\n",
        "\n",
        "# Create a DataFrame summarizing missing values after imputation\n",
        "na_summary = pd.DataFrame({\n",
        "    'Column': missing_values.index,\n",
        "    'Missing Values': missing_values.values,\n",
        "    'Percentage (%)': missing_percentager.values\n",
        "})\n",
        "\n",
        "# Filter columns with missing percentage > 0\n",
        "na_summary = na_summary[na_summary['Percentage (%)'] > 0].sort_values(by='Percentage (%)', ascending=True)\n",
        "\n",
        "# Display the summary\n",
        "print(\"Columns with Missing Values:\")\n",
        "print(na_summary)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xeaDCaGZBJ_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# droping average features with more than 30% missing\n",
        "avg_columns = na_summary[na_summary['Column'].str.contains(\"avg\", case=False)]['Column'].tolist()\n",
        "train.drop(columns=avg_columns, inplace=True)\n",
        "test.drop(columns=avg_columns, inplace=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "x4n9evqBCua_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we saw a unique thing. max_floor, num_room, material and kitch_sq have the exact same missing percentage, we belive that the missing rows are the same for all of them. lets check:\n",
        "\n",
        "target_columns = ['max_floor', 'num_room', 'kitch_sq', 'material']\n",
        "missing_ids = set(train.loc[train[target_columns].isna().any(axis=1), 'id'])\n",
        "num_unique_missing_ids = len(missing_ids)\n",
        "print(f\"Total number of unique IDs with missing values: {num_unique_missing_ids}\")\n",
        "\n",
        "# its indeed the same (10998 total missing rows - just like the number of missing rows for each of the features - meaning where one of the feature has na, all of them have na)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "UmleC8jfNJES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['full_sq', 'life_sq', 'floor', 'max_floor', 'material', 'build_year', 'num_room', 'kitch_sq', 'state', 'product_type', 'sub_area', 'log_price_doc', 'timestamp']\n",
        "house_price_missing = train.loc[train['id'].isin(missing_ids), features]\n",
        "\n",
        "# Display the first few rows\n",
        "print(house_price_missing.info())\n",
        "rows, columns = house_price_missing.shape\n",
        "print(f\"Number of rows: {rows}, Number of columns: {columns}\")\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "\n",
        "\n",
        "print(\"First 5 rows:\")\n",
        "display(house_price_missing.head())\n",
        "print(\"\\nTable Statistics:\")\n",
        "display(house_price_missing.describe())\n",
        "\n",
        "# after looking at the description, we saw that also state and build_year had na in the exact rows of the features in target_columns (and more rows with na).\n",
        "# this leads us to assume that they are also in that group, and they might share a common properties that can help us understand those houses better.\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QD7UA-NAVPtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.copy()\n",
        "house_price_missing = house_price_missing.copy()\n",
        "\n",
        "bins = [-np.inf] + list(range(1920, 2030, 20))\n",
        "bin_labels = [\"Before 1920\"] + [f\"{start}-{start+19}\" for start in range(1920, 2020, 20)]\n",
        "\n",
        "print(\"Bin Edges:\", bins)\n",
        "print(\"Bin Labels:\", bin_labels)\n",
        "\n",
        "# Filter out NaN values in build_year before binning\n",
        "train_plot = train.dropna(subset=['build_year']).copy()\n",
        "\n",
        "# Apply binning only to filtered data\n",
        "train_plot['build_year_bin'] = pd.cut(train_plot['build_year'], bins=bins, labels=bin_labels, include_lowest=True)\n",
        "\n",
        "# Convert to string format (no NaN values will remain)\n",
        "train_plot['build_year_bin'] = train_plot['build_year_bin'].astype(str)\n",
        "\n",
        "# Count occurrences in each bin\n",
        "bin_counts = train_plot['build_year_bin'].value_counts()\n",
        "print(\"Bin Distribution in train (filtered):\\n\", bin_counts)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pJEvp6KfpYy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_plot['timestamp'] = pd.to_datetime(train_plot['timestamp'])\n",
        "house_price_missing['timestamp'] = pd.to_datetime(house_price_missing['timestamp'])\n",
        "\n",
        "# Convert timestamps to \"YYYY-MM\" format for proper sorting\n",
        "train_plot['time_label'] = train_plot['timestamp'].dt.to_period('M').astype(str)\n",
        "house_price_missing['time_label'] = house_price_missing['timestamp'].dt.to_period('M').astype(str)\n",
        "\n",
        "# Add a special category for house_price_missing\n",
        "house_price_missing['build_year_bin'] = \"Missing Data\"\n",
        "\n",
        "# Combine datasets for visualization\n",
        "plot_data = pd.concat([train_plot[['time_label', 'log_price_doc', 'build_year_bin']],\n",
        "                       house_price_missing[['time_label', 'log_price_doc', 'build_year_bin']]])\n",
        "\n",
        "# Convert time_label back to datetime for correct sorting\n",
        "plot_data['time_label'] = pd.to_datetime(plot_data['time_label'], format='%Y-%m')\n",
        "\n",
        "# Sort properly by date\n",
        "plot_data = plot_data.sort_values(by='time_label')\n",
        "\n",
        "# Plot log_price_doc over time with build_year_bin as the legend\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.lineplot(data=plot_data, x='time_label', y='log_price_doc', hue='build_year_bin', marker=\"o\")\n",
        "\n",
        "plt.xticks(rotation=45)\n",
        "plt.xlabel(\"MM/YY\")\n",
        "plt.ylabel(\"Log Price Doc\")\n",
        "plt.title(\"Log Price Doc Over Time by Build Year Bin (Including Missing Data)\")\n",
        "plt.legend(title=\"Build Year Bins\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "uHvyGvJjy5wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_features = ['full_sq', 'life_sq', 'floor', 'log_price_doc']\n",
        "\n",
        "# Iterate over each bin and analyze log_price_doc distribution, correlation matrix, and metrics\n",
        "for bin_label in bin_labels:\n",
        "    print(f\"\\nAnalyzing Build Year Bin: {bin_label}\")\n",
        "\n",
        "    # Filter data for the current bin\n",
        "    bin_data = train_plot[train_plot['build_year_bin'] == bin_label]\n",
        "    # Ensure data is not empty before proceeding\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.kdeplot(bin_data['log_price_doc'], label=\"Full Data\", fill=True, color=\"blue\")\n",
        "    sns.kdeplot(house_price_missing['log_price_doc'], label=\"Missing Data\", fill=True, color=\"red\")\n",
        "    plt.title(f\"Distribution of log_price_doc for {bin_label}\")\n",
        "    plt.xlabel(\"Log Price Doc\")\n",
        "    plt.ylabel(\"Density\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    stats_full = {\n",
        "    \"Mean\": np.mean(bin_data['log_price_doc']),\n",
        "    \"Median\": np.median(bin_data['log_price_doc']),\n",
        "    \"Mode\": bin_data['log_price_doc'].mode().values[0] if not bin_data['log_price_doc'].mode().empty else np.nan,\n",
        "    \"Variance\": np.var(bin_data['log_price_doc'])\n",
        "    }\n",
        "\n",
        "    stats_missing = {\n",
        "        \"Mean\": np.mean(house_price_missing['log_price_doc']),\n",
        "        \"Median\": np.median(house_price_missing['log_price_doc']),\n",
        "        \"Mode\": house_price_missing['log_price_doc'].mode().values[0] if not house_price_missing['log_price_doc'].mode().empty else np.nan,\n",
        "        \"Variance\": np.var(house_price_missing['log_price_doc'])\n",
        "    }\n",
        "\n",
        "    print(f\"Statistics for {bin_label} (Full Data): {stats_full}\")\n",
        "    print(f\"Statistics for (Missing Data): {stats_missing}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EO20LolarkJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_filtered = train_original.copy()\n",
        "train_filtered = train_filtered.dropna(subset=['build_year'])\n",
        "# Calculate the minimum timestamp date\n",
        "min_date = train_filtered['timestamp'].min()\n",
        "\n",
        "# Display result\n",
        "print(\"Minimum timestamp in train after filtering NaN build_year:\", min_date)\n",
        "\n",
        "# Count rows before 2013-05-21 where build_year is NOT NaN\n",
        "pre_may_2013 = train[(train['timestamp'] < \"2013-05-21\") & (train['build_year'].notna())]\n",
        "\n",
        "# Display the result\n",
        "print(\"Rows before 2013-05-21 with a known build_year:\", len(pre_may_2013))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Lq0Ktmt-0eTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SUB-AREA ANALYSIS\n",
        "\n",
        "sub_area_missing = house_price_missing['sub_area'].value_counts()\n",
        "sub_area_all = train['sub_area'].value_counts()\n",
        "\n",
        "# Convert to percentage relative to each dataset\n",
        "sub_area_df = pd.DataFrame({\n",
        "    'All Data %': (sub_area_all / sub_area_all.sum()) * 100,\n",
        "    'Missing Data %': (sub_area_missing / sub_area_missing.sum()) * 100\n",
        "}).fillna(0)\n",
        "\n",
        "# Sort by overall presence in the dataset\n",
        "sub_area_df.sort_values(by='All Data %', ascending=False, inplace=True)\n",
        "\n",
        "# Plot missing percentage vs. total percentage per sub_area\n",
        "plt.figure(figsize=(14, 6))\n",
        "sub_area_df.head(30).plot(kind='bar', stacked=False, figsize=(14, 6), alpha=0.7)\n",
        "plt.title(\"Percentage of Records Per Sub Area (All Data vs. Missing Data)\")\n",
        "plt.xlabel(\"Sub Area\")\n",
        "plt.ylabel(\"Percentage (%)\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.legend([\"All Data %\", \"Missing Data %\"])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cmOvM5B4dvM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = ['product_type']\n",
        "\n",
        "for col in categorical_features:\n",
        "    # Get category distributions in missing data and full dataset\n",
        "    cat_missing = house_price_missing[col].value_counts(normalize=True) * 100\n",
        "    cat_all = train[col].value_counts(normalize=True) * 100\n",
        "\n",
        "    # Merge into a single DataFrame for comparison\n",
        "    cat_df = pd.DataFrame({'All Data %': cat_all, 'Missing Data %': cat_missing}).fillna(0)\n",
        "    cat_df.sort_values(by='All Data %', ascending=False, inplace=True)\n",
        "\n",
        "    # Plot categorical comparison\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    cat_df.plot(kind='bar', stacked=False, alpha=0.7)\n",
        "    plt.title(f\"Comparison of {col} Distribution (All Data vs. Missing Data)\")\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel(\"Percentage (%)\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend([\"All Data %\", \"Missing Data %\"])\n",
        "    plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DwHv-dcnkOAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate median distance to Kremlin for each region\n",
        "median_distance = train.groupby('sub_area')['kremlin_km'].median()\n",
        "\n",
        "# Calculate the average build_year for each region\n",
        "average_build_year = train.groupby('sub_area')['build_year'].mean()\n",
        "\n",
        "# Combine into a DataFrame\n",
        "region_stats = pd.DataFrame({'Median Distance to Kremlin': median_distance, 'Average Build Year': average_build_year})\n",
        "\n",
        "# Sort by median distance to Kremlin\n",
        "region_stats = region_stats.sort_values(by='Median Distance to Kremlin')\n",
        "\n",
        "# Plot distance and average build year\n",
        "fig, ax1 = plt.subplots(figsize=(20, 7))\n",
        "\n",
        "# Plot Median Distance to Kremlin\n",
        "color = 'tab:blue'\n",
        "ax1.set_xlabel('Sub Area')\n",
        "ax1.set_ylabel('Median Distance to Kremlin (km)', color=color)\n",
        "ax1.bar(region_stats.index, region_stats['Median Distance to Kremlin'], color=color, alpha=0.6, label=\"Median Distance\")\n",
        "ax1.tick_params(axis='y', labelcolor=color)\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Fit a trend line for Median Distance to Kremlin\n",
        "x_values = np.arange(len(region_stats))\n",
        "y_values = region_stats['Median Distance to Kremlin'].values\n",
        "z = np.polyfit(x_values, y_values, 1)  # Linear trend\n",
        "p = np.poly1d(z)\n",
        "ax1.plot(region_stats.index, p(x_values), color='blue', linestyle='dashed', label=\"Distance Trend\")\n",
        "\n",
        "# Plot Average Build Year on a second axis\n",
        "ax2 = ax1.twinx()\n",
        "color = 'tab:red'\n",
        "ax2.set_ylabel('Average Build Year', color=color)\n",
        "ax2.plot(region_stats.index, region_stats['Average Build Year'], color=color, marker='o', linestyle='dashed', label=\"Avg Build Year\")\n",
        "ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "# Fit a trend line for Average Build Year\n",
        "y_values2 = region_stats['Average Build Year'].values\n",
        "z2 = np.polyfit(x_values, y_values2, 1)  # Linear trend\n",
        "p2 = np.poly1d(z2)\n",
        "ax2.plot(region_stats.index, p2(x_values), color='red', linestyle='dotted', label=\"Build Year Trend\")\n",
        "\n",
        "# Title and legend\n",
        "plt.title(\"Median Distance to Kremlin vs. Average Build Year by Region (With Trend Lines)\")\n",
        "fig.tight_layout()\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cEGsJlpycBHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# few takes:\n",
        "# 1. transactions between 2011 to mid 2013 had no info about the year the house was built\n",
        "# 2. some regions had significantly more missing data in this time range\n",
        "# 3.OwnerOccupier also shoud sagnificant change in na values\n",
        "# 4. newer areas are Farther away to kremlin than older areas.\n",
        "# this leads us to think that maybe the missing data in build year is connected for houses in those regions"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xHOxUSs3jili"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# couldnt be sure that there is a connection, so we chose to imputate na with the method below. we conclude that until mid 2013\n",
        "# there was an issue entering the data in the features: max_floor, num_room, kitch_sq, material, state and build_year but couldnt trace it to a specific region or specific times wehre the house was built."
      ],
      "metadata": {
        "id": "Jvhni02F4X91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.loc[train[\"state\"].notna(), \"state\"] = train[\"state\"].dropna().astype(int)\n",
        "train.loc[train[\"material\"].notna(), \"material\"] = train[\"material\"].dropna().astype(int)\n",
        "\n",
        "# Print unique values to confirm conversion\n",
        "print(\"Unique values in state after conversion:\", train[\"state\"].dropna().unique())\n",
        "print(\"Unique values in material after conversion:\", train[\"material\"].dropna().unique())\n",
        "\n",
        "unique_states_counts = train[\"state\"].value_counts(dropna=True)\n",
        "unique_material_counts = train[\"material\"].value_counts(dropna=True)\n",
        "\n",
        "# Print unique values and their counts\n",
        "print(\"Unique values in state and their occurrences:\")\n",
        "print(unique_states_counts)\n",
        "print(\"Unique material in state and their occurrences:\")\n",
        "print(unique_material_counts)\n",
        "\n",
        "\n",
        "\n",
        "# probably a mistkae\n",
        "train.loc[train[\"state\"] == 33, \"state\"] = 3\n",
        "\n",
        "\n",
        "state_category_mapping = {\n",
        "    1: \"Poor\",\n",
        "    2: \"Decent\",\n",
        "    3: \"Good\",\n",
        "    4: \"Excellent\"\n",
        "}\n",
        "\n",
        "material_category_mapping = {\n",
        "    1: \"type1\",\n",
        "    2: \"type2\",\n",
        "    3: \"type3\",\n",
        "    4: \"type4\",\n",
        "    5: \"type5\",\n",
        "    6: \"type6\"\n",
        "}\n",
        "\n",
        "# Map state and material columns to categorical labels\n",
        "train[\"state\"] = train[\"state\"].map(state_category_mapping)\n",
        "train[\"material\"] = train[\"material\"].map(material_category_mapping)\n",
        "\n",
        "unique_states_counts = train[\"state\"].value_counts(dropna=True)\n",
        "unique_material_counts = train[\"material\"].value_counts(dropna=True)\n",
        "\n",
        "# Print unique values and their counts\n",
        "print(\"Unique values in state and their occurrences:\")\n",
        "print(unique_states_counts)\n",
        "print(\"Unique material in state and their occurrences:\")\n",
        "print(unique_material_counts)\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KmLJVNnbTTDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fill_mode_with_fallback(df, col, train_reference):\n",
        "    overall_mode = train_reference[col].mode()[0]  # Get mode from train\n",
        "\n",
        "    # Fill NA within each 'sub_area' group using mode from train_reference\n",
        "    df[col] = df.groupby('sub_area')[col].transform(\n",
        "        lambda x: x.fillna(train_reference.groupby('sub_area')[col].transform(lambda y: y.mode()[0] if not y.mode().empty else np.nan))\n",
        "    )\n",
        "\n",
        "    # Fill remaining NA with overall mode from train_reference\n",
        "    df[col] = df[col].fillna(overall_mode)\n",
        "    return df\n",
        "\n",
        "# Apply to Train (Original Logic)\n",
        "train = fill_mode_with_fallback(train, 'state', train)\n",
        "train = fill_mode_with_fallback(train, 'material', train)\n",
        "\n",
        "# Apply to Test using Train Statistics\n",
        "test = fill_mode_with_fallback(test, 'state', train)\n",
        "test = fill_mode_with_fallback(test, 'material', train)\n",
        "\n",
        "# Check if missing values remain\n",
        "print(\"Missing values after imputation in Train:\")\n",
        "print(train[['state', 'material']].isna().sum())\n",
        "\n",
        "print(\"\\nMissing values after imputation in Test:\")\n",
        "print(test[['state', 'material']].isna().sum())\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qGwFSy9sI5oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of categorical columns to plot\n",
        "categorical_columns = ['material', 'state']\n",
        "\n",
        "# Loop through categorical columns and plot count & box plots\n",
        "for col in categorical_columns:\n",
        "    plt.figure(figsize=(16, 6))\n",
        "\n",
        "    # Count Plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.countplot(data=train, x=col, palette='viridis', hue=col, order=train[col].value_counts().index)\n",
        "    plt.title(f'Distribution of {col}', fontsize=14)\n",
        "    plt.xlabel(col, fontsize=12)\n",
        "    plt.ylabel('Count', fontsize=12)\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Box Plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.boxplot(data=train, x=col, y=\"log_price_doc\", palette='viridis', hue=col, order=train[col].value_counts().index)\n",
        "    plt.title(f'Box Plot of log_price_doc for {col}', fontsize=14)\n",
        "    plt.xlabel(col, fontsize=12)\n",
        "    plt.ylabel('log_price_doc', fontsize=12)\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PkcETo6sTUyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure grouping features are of the same type in both train and test\n",
        "grouping_features = ['ID_metro', 'sub_area', 'material']\n",
        "\n",
        "for col in grouping_features:\n",
        "    train[col] = train[col].astype(str)\n",
        "    test[col] = test[col].astype(str)\n",
        "\n",
        "# Define function for test imputation based on train\n",
        "def impute_test_based_on_train(test_df, train_df, col, group_cols):\n",
        "\n",
        "    # Compute median values from train and store in dictionary\n",
        "    median_map = train_df.groupby(group_cols)[col].median().to_dict()\n",
        "\n",
        "    # Apply median imputation based on matching group\n",
        "    test_df[col] = test_df.apply(\n",
        "        lambda row: median_map.get(tuple(row[group_cols]), np.nan) if pd.isna(row[col]) else row[col], axis=1\n",
        "    )\n",
        "\n",
        "    # Fill remaining missing values with global median from train\n",
        "    test_df[col] = test_df[col].fillna(train_df[col].median())\n",
        "    return test_df\n",
        "\n",
        "# Impute Train Data First\n",
        "for col in ['floor', 'max_floor', 'kitch_sq', 'life_sq', 'num_room', 'build_year']:\n",
        "    if train[col].notna().sum() > 0:  # Only apply if train has valid data\n",
        "        train[col] = train.groupby(grouping_features)[col].transform(lambda x: x.fillna(x.median()))\n",
        "\n",
        "    # Fill any remaining NA with overall median of train\n",
        "    train[col] = train[col].fillna(train[col].median())\n",
        "\n",
        "# Apply Imputation to Test Data Using Train\n",
        "for col in ['floor', 'max_floor', 'kitch_sq', 'life_sq', 'num_room']:\n",
        "    test = impute_test_based_on_train(test, train, col, grouping_features)\n",
        "\n",
        "# Final Check: Print Missing Values After Imputation\n",
        "print(\"Missing values after imputation in Train:\")\n",
        "print(train[['floor', 'max_floor', 'kitch_sq', 'life_sq', 'num_room']].isna().sum())\n",
        "\n",
        "print(\"\\nMissing values after imputation in Test:\")\n",
        "print(test[['floor', 'max_floor', 'kitch_sq', 'life_sq', 'num_room']].isna().sum())\n"
      ],
      "metadata": {
        "id": "G9rfzQqIBegm",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values = train.isna().sum()\n",
        "missing_percentager = (missing_values / len(train)) * 100\n",
        "\n",
        "# Create a DataFrame summarizing missing values after imputation\n",
        "na_summary = pd.DataFrame({\n",
        "    'Column': missing_values.index,\n",
        "    'Missing Values': missing_values.values,\n",
        "    'Percentage (%)': missing_percentager.values\n",
        "})\n",
        "\n",
        "# Filter columns with missing percentage > 0\n",
        "na_summary = na_summary[na_summary['Percentage (%)'] > 0].sort_values(by='Percentage (%)', ascending=True)\n",
        "\n",
        "# Display the summary\n",
        "print(\"Columns with Missing Values:\")\n",
        "print(na_summary)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "lnYoi-RZcDcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from geopy.geocoders import Nominatim\n",
        "import time\n",
        "\n",
        "# Initialize geolocator\n",
        "geolocator = Nominatim(user_agent=\"geo_locator\", timeout=10)\n",
        "\n",
        "# Function to get coordinates for each sub_area\n",
        "def get_coordinates(sub_area):\n",
        "    try:\n",
        "        location = geolocator.geocode(f\"{sub_area}, Moscow, Russia\")\n",
        "        if location:\n",
        "            return (location.latitude, location.longitude)\n",
        "    except Exception as e:\n",
        "        print(f\"Error retrieving coordinates for {sub_area}: {e}\")\n",
        "    return (None, None)\n",
        "\n",
        "# Group by sub_area and calculate the mean log_price_doc\n",
        "sub_area_prices = train.groupby('sub_area')['price_doc'].median().reset_index()\n",
        "\n",
        "# Get coordinates for each sub_area\n",
        "sub_area_prices[['Latitude', 'Longitude']] = sub_area_prices['sub_area'].apply(\n",
        "    lambda x: pd.Series(get_coordinates(x))\n",
        ")\n",
        "\n",
        "# Remove rows where coordinates are missing\n",
        "sub_area_prices = sub_area_prices.dropna()\n",
        "\n",
        "# Create a Folium map centered around Moscow\n",
        "moscow_map = folium.Map(location=[55.751244, 37.618423], zoom_start=10)\n",
        "\n",
        "# Create a heatmap layer\n",
        "heat_data = sub_area_prices[['Latitude', 'Longitude', 'price_doc']].values.tolist()\n",
        "HeatMap(heat_data, radius=15, blur=10, max_zoom=1).add_to(moscow_map)\n",
        "\n",
        "\n",
        "# Display the map\n",
        "moscow_map\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9drGEn8cXMPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_correlation_matrix(df, group, group_name):\n",
        "    # Ensure all selected columns are numeric\n",
        "    numeric_df = df[group].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "    # Drop columns with all NaN values\n",
        "    numeric_df = numeric_df.dropna(axis=1, how='all')\n",
        "\n",
        "    if numeric_df.empty:\n",
        "        print(f\"No valid numeric columns for {group_name}\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    correlation_matrix = numeric_df.corr()\n",
        "    sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', linewidths=0.5)\n",
        "    plt.title(f'Correlation Matrix: {group_name}')\n",
        "    plt.show()\n",
        "\n",
        "# Define column groups\n",
        "groups = {\n",
        "    \"Group 1\": [\"area_m\", \"raion_popul\", \"green_zone_part\", \"indust_part\", \"children_preschool\", \"preschool_quota\",\n",
        "                \"preschool_education_centers_raion\", \"children_school\", \"school_quota\", \"school_education_centers_raion\",\n",
        "                \"school_education_centers_top_20_raion\", \"hospital_beds_raion\", \"healthcare_centers_raion\",\n",
        "                \"university_top_20_raion\", \"sport_objects_raion\", \"additional_education_raion\", \"culture_objects_top_25\",\n",
        "                \"culture_objects_top_25_raion\", \"shopping_centers_raion\", \"office_raion\"],\n",
        "    \"Group 2\": [\"full_all\", \"male_f\", \"female_f\", \"young_all\", \"young_male\", \"young_female\", \"work_all\", \"work_male\", \"work_female\",\n",
        "                \"ekder_all\", \"ekder_male\", \"ekder_female\", \"0_6_all\", \"0_6_male\", \"0_6_female\", \"7_14_all\", \"7_14_male\", \"7_14_female\",\n",
        "                \"0_17_all\", \"0_17_male\", \"0_17_female\", \"16_29_all\", \"16_29_male\", \"16_29_female\", \"0_13_all\", \"0_13_male\", \"0_13_female\"],\n",
        "    \"Group 3\": [\"raion_build_count_with_material_info\", \"build_count_block\", \"build_count_wood\", \"build_count_frame\", \"build_count_brick\",\n",
        "                \"build_count_monolith\", \"build_count_panel\", \"build_count_foam\", \"build_count_slag\", \"build_count_mix\",\n",
        "                \"raion_build_count_with_builddate_info\", \"build_count_before_1920\", \"build_count_1921-1945\", \"build_count_1946-1970\",\n",
        "                \"build_count_1971-1995\", \"build_count_after_1995\"],\n",
        "    \"Group 4\": [\"metro_min_avto\", \"metro_km_avto\", \"metro_min_walk\", \"metro_km_walk\", \"kindergarten_km\", \"school_km\", \"park_km\", \"green_zone_km\",\n",
        "                \"industrial_km\", \"water_treatment_km\", \"cemetery_km\", \"incineration_km\", \"railroad_station_walk_km\", \"railroad_station_walk_min\",\n",
        "                \"ID_railroad_station_walk\", \"railroad_station_avto_km\", \"railroad_station_avto_min\", \"ID_railroad_station_avto\",\n",
        "                \"public_transport_station_km\", \"public_transport_station_min_walk\", \"water_km\", \"water_1line\", \"mkad_km\", \"ttk_km\", \"sadovoe_km\",\n",
        "                \"bulvar_ring_km\", \"kremlin_km\", \"big_road1_km\", \"ID_big_road1\", \"big_road1_1line\", \"big_road2_km\", \"ID_big_road2\", \"railroad_km\",\n",
        "                \"railroad_1line\", \"zd_vokzaly_avto_km\", \"ID_railroad_terminal\", \"bus_terminal_avto_km\", \"ID_bus_terminal\"],\n",
        "    \"Group 5\": [\"oil_chemistry_km\", \"nuclear_reactor_km\", \"radiation_km\", \"power_transmission_line_km\", \"thermal_power_plant_km\", \"ts_km\",\n",
        "                \"big_market_km\", \"market_shop_km\", \"fitness_km\", \"swim_pool_km\", \"ice_rink_km\", \"stadium_km\", \"basketball_km\", \"hospice_morgue_km\",\n",
        "                \"detention_facility_km\", \"public_healthcare_km\", \"university_km\", \"workplaces_km\", \"shopping_centers_km\", \"office_km\",\n",
        "                \"additional_education_km\", \"preschool_km\", \"big_church_km\", \"church_synagogue_km\", \"mosque_km\", \"theater_km\", \"museum_km\",\n",
        "                \"exhibition_km\", \"catering_km\"],\n",
        "    \"Group 6\": [col for col in train.columns if \"500\" in col or \"1000\" in col or \"1500\" in col or \"2000\" in col or \"3000\" in col or \"5000\" in col]\n",
        "}\n",
        "\n",
        "# Plot correlation matrices\n",
        "for group_name, group_columns in groups.items():\n",
        "    valid_columns = [col for col in group_columns if col in train.columns]  # Ensure columns exist\n",
        "    if valid_columns:\n",
        "        plot_correlation_matrix(train, valid_columns, group_name)\n",
        "    else:\n",
        "        print(f\"No valid columns for {group_name}\")\n"
      ],
      "metadata": {
        "id": "T81iHXdSnAXj",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define groups based on the images\n",
        "feature_groups = {\n",
        "    \"Group 1\": [\"children_preschool\", \"preschool_quota\", \"preschool_education_centers_raion\",\n",
        "                \"children_school\", \"school_quota\"],\n",
        "    \"Group 2\": [\"0_6_all\", \"0_6_male\", \"0_6_female\", \"7_14_all\", \"7_14_male\", \"7_14_female\",\n",
        "                \"0_17_all\", \"0_17_male\", \"0_17_female\"],\n",
        "    \"Group 3\": [\"ttk_km\", \"sadovoe_km\", \"bulvar_ring_km\"],\n",
        "    \"Group 4\": [\"metro_min_avto\", \"metro_km_avto\", \"metro_min_walk\", \"metro_km_walk\"],\n",
        "    \"Group 5\": [\"kremlin_km\"]  # Delete all and keep only kremlin_km\n",
        "}\n",
        "\n",
        "# Flatten the list of features to be considered\n",
        "all_features = [feature for group in feature_groups.values() for feature in group]\n",
        "\n",
        "# Ensure only these features exist in the DataFrame before computing correlations\n",
        "valid_features = [feature for feature in all_features if feature in train.columns]\n",
        "\n",
        "# Compute correlations only for the valid features\n",
        "correlations = train[valid_features].corrwith(train[\"log_price_doc\"]).abs()\n",
        "\n",
        "# Select the highest correlated feature in each group (except Group 5 where we keep only kremlin_km)\n",
        "selected_features = []\n",
        "\n",
        "for group, features in feature_groups.items():\n",
        "    valid_group_features = [feature for feature in features if feature in train.columns]\n",
        "    if group == \"Group 5\":  # Keep only kremlin_km\n",
        "        selected_features.append(\"kremlin_km\")\n",
        "    elif valid_group_features:  # Check if the group has valid features\n",
        "        highest_corr_feature = correlations[valid_group_features].idxmax()\n",
        "        selected_features.append(highest_corr_feature)\n",
        "\n",
        "# Drop only the unselected features from feature_groups, keeping all other columns in train\n",
        "features_to_drop = set(all_features) - set(selected_features)\n",
        "train.drop(columns=features_to_drop, inplace=True, errors=\"ignore\")\n",
        "test.drop(columns=features_to_drop, inplace=True, errors=\"ignore\")\n",
        "\n",
        "# Print the selected and dropped features\n",
        "print(\"Selected features:\", selected_features)\n",
        "print(\"Dropped features:\", features_to_drop)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "066aFFDSdeE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.drop(columns=[col for col in train.columns if 'ID' in col])\n",
        "test = test.drop(columns=[col for col in test.columns if 'ID' in col])\n",
        "\n",
        "\n",
        "train[\"timestamp\"] = pd.to_datetime(train[\"timestamp\"])\n",
        "train[\"year\"] = train[\"timestamp\"].dt.year\n",
        "train[\"month\"] = train[\"timestamp\"].dt.month\n",
        "train[\"year_month\"] = train[\"timestamp\"].dt.to_period(\"M\").astype(str)\n",
        "\n",
        "test[\"timestamp\"] = pd.to_datetime(test[\"timestamp\"])\n",
        "test[\"year\"] = test[\"timestamp\"].dt.year\n",
        "test[\"month\"] = test[\"timestamp\"].dt.month\n",
        "test[\"year_month\"] = test[\"timestamp\"].dt.to_period(\"M\").astype(str)\n",
        "\n"
      ],
      "metadata": {
        "id": "a3bfnrmBQPaA",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_feature_engineering(df):\n",
        "    df['resident_to_total_ratio'] = df['life_sq'] / df['full_sq']\n",
        "    df['kitchen_to_total_ratio'] = df['kitch_sq'] / df['full_sq']\n",
        "    df['avg_room_area'] = df['life_sq'] / df['num_room']\n",
        "\n",
        "    # Demographic structures of subareas:\n",
        "    df['young_proportion'] = df['young_all'] / df['full_all']\n",
        "    df['work_proportion'] = df['work_all'] / df['full_all']\n",
        "    df['retire_proportion'] = df['ekder_all'] / df['full_all']\n",
        "    df['female_to_male'] = df['female_f'] / df['male_f']\n",
        "\n",
        "    # Flags for building size\n",
        "    df['large_flag'] = np.where(df['max_floor'] >= 20, 1, 0)\n",
        "    df['small_flag'] = np.where(df['max_floor'] <= 20, 1, 0)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply feature engineering to both train and test datasets\n",
        "train = apply_feature_engineering(train)\n",
        "test = apply_feature_engineering(test)\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nYdM9QoOSXUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# noticed some values in product type were nan\n",
        "\n",
        "unique_values = test['product_type'].unique()\n",
        "value_counts = test['product_type'].value_counts(dropna=False)\n",
        "\n",
        "print(\"Unique values in 'product_type' column:\")\n",
        "print(unique_values)\n",
        "print(\"Count of each unique value (including NaN):\")\n",
        "print(value_counts)\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "W2GNkH1xOgxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['product_type'].fillna('Investment', inplace=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MDeTjIKhOwZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_before_running_model = test.copy()"
      ],
      "metadata": {
        "id": "qcLHxAuCr1ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Drop price_doc if it exists\n",
        "if 'price_doc' in train.columns:\n",
        "    train = train.drop(columns=['price_doc'])\n",
        "\n",
        "# Define the target variable\n",
        "target_column = \"log_price_doc\"\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = train.drop(columns=[target_column].copy())\n",
        "\n",
        "# Exclude timestamp and id if they exist\n",
        "excluded_cols = ['id', 'timestamp']\n",
        "X = X.drop(columns=[col for col in excluded_cols if col in X.columns], errors=\"ignore\")\n",
        "\n",
        "# Define target variable\n",
        "y = train[target_column]\n",
        "\n",
        "# Identify categorical and numerical features\n",
        "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "numerical_features = X.select_dtypes(include=['number']).columns.tolist()\n",
        "\n",
        "# Define transformers for numerical and categorical data\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())  # Scale numerical features\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # One-hot encode categorical features\n",
        "])\n",
        "\n",
        "# Apply transformations\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer, numerical_features),\n",
        "    ('cat', categorical_transformer, categorical_features)\n",
        "])\n",
        "\n",
        "# Transform the dataset\n",
        "X_prepared = preprocessor.fit_transform(X)\n",
        "\n",
        "# Convert to DataFrame with proper column names\n",
        "encoded_cat_features = preprocessor.named_transformers_['cat'].named_steps['encoder'].get_feature_names_out(categorical_features)\n",
        "processed_features = numerical_features + encoded_cat_features.tolist()\n",
        "X_prepared_df = pd.DataFrame(X_prepared, columns=processed_features, index=X.index)\n",
        "\n",
        "# Print summary\n",
        "print(f\"Data prepared for XGBoost. Shape: {X_prepared_df.shape}\")\n"
      ],
      "metadata": {
        "id": "GpTZ43tlh0FX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "# Convert X_prepared_df to a NumPy array for training\n",
        "X_train_array = X_prepared_df.values  # Convert to NumPy array\n",
        "y_train_array = y.values  # Ensure y is also a NumPy array\n",
        "\n",
        "# Define the XGBoost model\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    objective=\"reg:squarederror\",\n",
        "    n_estimators=100,  # Number of boosting rounds\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the XGBoost model\n",
        "print(\"Training XGBoost model on selected features...\")\n",
        "xgb_model.fit(X_train_array, y_train_array)\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importances = xgb_model.feature_importances_\n",
        "\n",
        "# Convert to DataFrame for easy sorting\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': X_prepared_df.columns,\n",
        "    'Importance': feature_importances\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(feature_importance_df['Feature'][:20], feature_importance_df['Importance'][:20], color='skyblue')\n",
        "plt.gca().invert_yaxis()  # Highest importance at the top\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.ylabel(\"Feature Name\")\n",
        "plt.title(\"Top 20 Features - XGBoost Feature Importance\")\n",
        "plt.show()\n",
        "\n",
        "# Select the top N important features (adjust N as needed)\n",
        "N = 200  # Change this value to select a different number of top features\n",
        "top_features = feature_importance_df['Feature'][:N].tolist()\n",
        "\n",
        "# Ensure only existing features are kept\n",
        "features_to_keep = [feature for feature in top_features if feature in X_prepared_df.columns]\n",
        "\n",
        "# Create a new DataFrame with selected features\n",
        "train_selected = pd.concat([X_prepared_df[features_to_keep], y], axis=1)\n",
        "\n",
        "# Print summary\n",
        "print(f\"Selected top {N} features using XGBoost feature selection.\")\n",
        "print(features_to_keep)\n",
        "\n",
        "# Overwrite train with only selected features\n",
        "train = train_selected.copy()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "a3VauVhZjcrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['id'] = test_original['id']\n",
        "test_ids = test[['id']].copy()\n",
        "\n",
        "\n",
        "excluded_cols = ['timestamp', 'id']\n",
        "X_test = test.drop(columns=[col for col in excluded_cols if col in test.columns], errors=\"ignore\")\n",
        "\n",
        "# Apply the Same Preprocessing as Train\n",
        "X_test_prepared = preprocessor.transform(X_test)  # Transform test using fitted preprocessor\n",
        "\n",
        "# Convert transformed data back into DataFrame\n",
        "X_test_prepared_df = pd.DataFrame(X_test_prepared, columns=processed_features, index=test.index)\n",
        "\n",
        "# Ensure Test Columns Match Train (Fix Any Missing Encoded Features)\n",
        "missing_features = set(X_prepared_df.columns) - set(X_test_prepared_df.columns)\n",
        "for feature in missing_features:\n",
        "    X_test_prepared_df[feature] = 0  # Fill missing features with 0 to match training structure\n",
        "\n",
        "# Ensure Column Order is Identical to Train\n",
        "X_test_prepared_df = X_test_prepared_df[X_prepared_df.columns]\n",
        "\n",
        "# Make Predictions\n",
        "y_test_pred = xgb_model.predict(X_test_prepared_df)\n",
        "\n",
        "# Restore 'id' and Create Submission\n",
        "test['log_price_doc'] = y_test_pred\n",
        "test['price_doc'] = np.expm1(y_test_pred)  # Reverse log transformation\n",
        "\n",
        "submission = test[['id', 'price_doc']].set_index('id').reindex(test_ids['id']).reset_index()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vwnokj-Kbd-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the file name\n",
        "submission_file = \"submission.csv\"\n",
        "\n",
        "# Save the submission DataFrame as an Excel file\n",
        "submission.to_csv(submission_file, index=False)\n",
        "\n",
        "# Provide a download link\n",
        "from google.colab import files\n",
        "files.download(submission_file)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "o360hE4We5tH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_owner = X_prepared_df[X_prepared_df[\"product_type_OwnerOccupier\"] == 1]\n",
        "x_investment = X_prepared_df[X_prepared_df[\"product_type_Investment\"] == 1]\n",
        "print(f\"OwnerOccupier Data: {x_owner.shape}, Investment Data: {x_investment.shape}\")\n",
        "y = train[target_column]\n",
        "y_owner = y[X_prepared_df[\"product_type_OwnerOccupier\"] == 1]\n",
        "y_investment = y[X_prepared_df[\"product_type_Investment\"] == 1]\n",
        "print(f\"OwnerOccupier Data: {y_owner.shape}, Investment Data: {y_investment.shape}\")\n",
        "\n",
        "\n",
        "def train_xgboost(X, y, title):\n",
        "    model = xgb.XGBRegressor(\n",
        "        objective=\"reg:squarederror\",\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=6,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    print(f\"Training XGBoost model for {title} properties...\")\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Get predictions\n",
        "    y_pred = model.predict(X)\n",
        "\n",
        "    # Compute residuals\n",
        "    residuals = y - y_pred\n",
        "\n",
        "    # Get feature importance\n",
        "    feature_importances = model.feature_importances_\n",
        "    importance_df = pd.DataFrame({\"Feature\": X.columns, \"Importance\": feature_importances})\n",
        "    importance_df = importance_df.sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.barh(importance_df[\"Feature\"][:20], importance_df[\"Importance\"][:20], color=\"skyblue\")\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.xlabel(\"Feature Importance\")\n",
        "    plt.ylabel(\"Feature Name\")\n",
        "    plt.title(f\"Top 20 Features - XGBoost ({title})\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # QQ Plot\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
        "    plt.title(f\"QQ Plot of Residuals - {title}\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Histogram of Residuals\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.histplot(residuals, bins=50, kde=True, color=\"blue\", edgecolor=\"black\")\n",
        "    plt.axvline(x=0, color=\"red\", linestyle=\"--\")  # Reference line at zero\n",
        "    plt.xlabel(\"Residuals\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.title(f\"Histogram of Residuals - {title}\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "xgb_owner = train_xgboost(x_owner, y_owner, \"OwnerOccupier\")\n",
        "xgb_investment = train_xgboost(x_investment, y_investment, \"Investment\")\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "f6vi6TKwtCeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_owner = X_test_prepared_df[X_test_prepared_df[\"product_type_OwnerOccupier\"] == 1]\n",
        "test_investment = X_test_prepared_df[X_test_prepared_df[\"product_type_Investment\"] == 1]\n",
        "\n",
        "# Print summary\n",
        "print(f\"Successfully split test data into two subsets:\")\n",
        "print(f\"OwnerOccupier Test Data: {test_owner.shape}\")\n",
        "print(f\"Investment Test Data: {test_investment.shape}\")\n",
        "\n",
        "print(f\"Investment Test Data: {X_test_prepared_df.shape}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "44g6EDXtzZ9Y",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_features = xgb_owner.get_booster().feature_names\n",
        "test_owner_subset = test_owner[training_features]\n",
        "y_test_pred_owner = xgb_owner.predict(test_owner_subset)\n",
        "\n",
        "training_features_investment = xgb_investment.get_booster().feature_names\n",
        "test_investment_subset = test_investment[training_features_investment]\n",
        "y_test_pred_investment = xgb_investment.predict(test_investment_subset)\n",
        "\n"
      ],
      "metadata": {
        "id": "Xl3k0JaySODy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_owner[\"log_price_doc\"] = y_test_pred_owner\n",
        "test_investment[\"log_price_doc\"] = y_test_pred_investment\n",
        "\n",
        "test_owner[\"price_doc\"] = np.expm1(test_owner[\"log_price_doc\"])\n",
        "test_investment[\"price_doc\"] = np.expm1(test_investment[\"log_price_doc\"])\n",
        "\n",
        "test_owner[\"id\"] = test_before_running_model.loc[test_owner.index, \"id\"]\n",
        "test_investment[\"id\"] = test_before_running_model.loc[test_investment.index, \"id\"]\n",
        "\n",
        "final_predictions = pd.concat([test_owner[[\"id\", \"price_doc\"]], test_investment[[\"id\", \"price_doc\"]]])\n",
        "\n",
        "test_ids = test_before_running_model[[\"id\"]].copy()\n",
        "submission = final_predictions.set_index(\"id\").reindex(test_ids[\"id\"]).reset_index()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9EXOchVGTBcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_file = \"submission2.csv\"\n",
        "submission.to_csv(submission_file, index=False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(submission_file)\n",
        "\n",
        "print(f\"Predictions saved successfully: {submission_file}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eCcuapHtTHCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upload predictions to kaggle and recieved: Score: 0.34079, Private score: 0.34304 which is slighly higher than before. this can happen due to malformed data in investment prices.\n",
        "# the general model captures the \"whole image\" and manages to be somewhat accurate overall, but when we divide to product types - the investment model under performes (can be seen with the residuales graphs)\n",
        "# so we have decided to use the better model to identify outliers in predictions, and delete those rows in the train set.\n",
        "# by doing that we improve the predictions of the investment model.\n"
      ],
      "metadata": {
        "id": "7Ot0KkpDVNl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "\n",
        "# Predict prices for Investment properties using the OwnerOccupier model\n",
        "y_pred_investment = xgb_owner.predict(x_investment)\n",
        "print(\"shape of y_pred_investment: \", y_pred_investment.shape)\n",
        "print(\"shape of y_investment: \", y_investment.shape)\n",
        "# Compute residuals\n",
        "investment_residuals = y_investment - y_pred_investment\n",
        "\n",
        "# Add residuals and timestamp to a DataFrame for analysis\n",
        "investment_residuals_df = pd.DataFrame({\n",
        "    \"residuals\": investment_residuals,\n",
        "    \"predicted_log_price\": y_pred_investment\n",
        "})\n",
        "\n",
        "# Plot histogram of residuals\n",
        "plt.figure(figsize=(12, 5))\n",
        "sns.histplot(investment_residuals, bins=30, kde=True, color=\"blue\")\n",
        "plt.axvline(x=0, color=\"red\", linestyle=\"--\")  # Line at 0 residual\n",
        "plt.title(\"Residual Histogram - Investment Properties (Predicted by Owner Model)\")\n",
        "plt.xlabel(\"Residual Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n",
        "# Scatter plot of residuals vs. predicted values\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.scatter(y_pred_investment, investment_residuals, alpha=0.5, color=\"blue\")\n",
        "plt.axhline(y=0, color=\"red\", linestyle=\"--\")  # Line at 0 residual\n",
        "plt.title(\"Residuals vs. Predicted Log Price - Investment Properties\")\n",
        "plt.xlabel(\"Predicted Log Price\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.show()\n",
        "\n",
        "# QQ Plot to check normality of residuals\n",
        "plt.figure(figsize=(6, 6))\n",
        "stats.probplot(investment_residuals, dist=\"norm\", plot=plt)\n",
        "plt.title(\"QQ Plot of Residuals - Investment Properties\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "HsCsDS0pnVKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict prices for Investment properties using the OwnerOccupier model\n",
        "y_pred_investment = xgb_owner.predict(x_investment)\n",
        "\n",
        "# Compute residuals\n",
        "investment_residuals = y_investment - y_pred_investment\n",
        "\n",
        "# Define the outlier threshold using IQR (Interquartile Range)\n",
        "Q1 = np.percentile(investment_residuals, 25)\n",
        "Q3 = np.percentile(investment_residuals, 75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define acceptable residual range (1.5 * IQR rule)\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Identify outliers\n",
        "outliers = (investment_residuals < lower_bound) | (investment_residuals > upper_bound)\n",
        "num_outliers = outliers.sum()\n",
        "\n",
        "\n",
        "train_investment_cleaned = x_investment.loc[~outliers].copy()\n",
        "investment_residuals_cleaned = investment_residuals[~outliers]\n",
        "y_pred_investment_cleaned = y_pred_investment[~outliers]\n",
        "\n",
        "# Print summary of removed rows\n",
        "print(f\"Removed {num_outliers} outliers based on residuals.\")\n",
        "\n",
        "# Re-run Residual Plots after Outlier Removal\n",
        "plt.figure(figsize=(12, 5))\n",
        "sns.histplot(investment_residuals_cleaned, bins=30, kde=True, color=\"blue\")\n",
        "plt.axvline(x=0, color=\"red\", linestyle=\"--\")\n",
        "plt.title(\"Residual Histogram (After Outlier Removal) - Investment Properties\")\n",
        "plt.xlabel(\"Residual Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.scatter(y_pred_investment_cleaned, investment_residuals_cleaned, alpha=0.5, color=\"blue\")\n",
        "plt.axhline(y=0, color=\"red\", linestyle=\"--\")\n",
        "plt.title(\"Residuals vs. Predicted Log Price (After Outlier Removal) - Investment Properties\")\n",
        "plt.xlabel(\"Predicted Log Price\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nt70q7MOoreM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_owner = X_prepared_df[X_prepared_df[\"product_type_OwnerOccupier\"] == 1].copy()\n",
        "X_investment = X_prepared_df[X_prepared_df[\"product_type_Investment\"] == 1].copy()\n",
        "\n",
        "y_owner = y.loc[X_owner.index]\n",
        "y_investment = y.loc[X_investment.index]\n",
        "\n",
        "print(f\"Data Sizes:\")\n",
        "print(f\"OwnerOccupier: {X_owner.shape}, {y_owner.shape}\")\n",
        "print(f\"Investment: {X_investment.shape}, {y_investment.shape}\")\n",
        "\n",
        "x_investment = X_investment.copy()\n",
        "\n",
        "# Identify Outliers\n",
        "outliers_investment = (investment_residuals < lower_bound) | (investment_residuals > upper_bound)\n",
        "\n",
        "\n",
        "outlier_ids = y_investment.index[outliers_investment]\n",
        "\n",
        "# Remove outliers from BOTH `X_investment` and `y_investment`\n",
        "X_investment_cleaned = X_investment.drop(index=outlier_ids)\n",
        "y_investment_cleaned = y_investment.drop(index=outlier_ids)\n",
        "\n",
        "# Align to ensure perfect shape match\n",
        "x_investment, y_investment = X_investment_cleaned.align(y_investment_cleaned, axis=0, join=\"inner\")\n",
        "\n",
        "\n",
        "print(f\"Final Data Sizes:\")\n",
        "print(f\"OwnerOccupier: {x_owner.shape}, {y_owner.shape}\")\n",
        "print(f\"Investment (cleaned): {x_investment.shape}, {y_investment.shape}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "uFgWP_Xkkk3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna"
      ],
      "metadata": {
        "collapsed": true,
        "id": "o6ZQN0_JR2hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import optuna\n",
        "\n",
        "\n",
        "# Define Time-Series Cross-Validation\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "# Store trial results for later analysis\n",
        "trial_results = []\n",
        "\n",
        "# Function for Optuna Optimization with Time-Series CV\n",
        "def optimize_xgboost(trial, X, y, title):\n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 400),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.05, 0.2, log=True),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1),\n",
        "        \"random_state\": 14\n",
        "    }\n",
        "\n",
        "    rmse_scores = []  # Store RMSE for each fold\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        # Train XGBoost model\n",
        "        model = xgb.XGBRegressor(**params)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions & compute RMSE\n",
        "        y_pred = model.predict(X_val)\n",
        "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "        rmse_scores.append(rmse)\n",
        "\n",
        "        # Print real-time RMSE for each fold\n",
        "        print(f\"Trial {trial.number} | {title} | Fold {fold+1} | RMSE: {rmse:.4f}\")\n",
        "\n",
        "    # Compute average RMSE across folds\n",
        "    avg_rmse = np.mean(rmse_scores)\n",
        "\n",
        "    #  Print final RMSE for this trial\n",
        "    print(f\"Trial {trial.number} Completed | {title} | Average RMSE: {avg_rmse:.4f}\")\n",
        "\n",
        "    # Save trial details for later analysis\n",
        "    trial_results.append({\"trial\": trial.number, \"rmse\": avg_rmse, **params})\n",
        "\n",
        "    return avg_rmse  # Optuna will minimize this value\n",
        "\n",
        "# Train XGBoost with Optuna Optimization (Now Using Time-Series CV)\n",
        "def train_xgboost_with_optuna(X, y, title):\n",
        "    study = optuna.create_study(direction=\"minimize\")\n",
        "    study.optimize(lambda trial: optimize_xgboost(trial, X, y, title), n_trials=20)\n",
        "    best_params = study.best_params\n",
        "    best_model = xgb.XGBRegressor(**best_params)\n",
        "    best_model.fit(X, y)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = best_model.predict(X)\n",
        "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
        "\n",
        "    # Plot predicted vs actual values\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.scatter(y, y_pred, alpha=0.5, color=\"blue\")\n",
        "    plt.plot([y.min(), y.max()], [y.min(), y.max()], color=\"red\", linestyle=\"--\")\n",
        "    plt.xlabel(\"Actual Log Price\")\n",
        "    plt.ylabel(\"Predicted Log Price\")\n",
        "    plt.title(f\"Actual vs. Predicted Log Price - {title}\")\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Best Parameters for {title}: {best_params}\")\n",
        "    print(f\"RMSE for {title}: {rmse:.4f}\")\n",
        "\n",
        "    return best_model, y_pred, rmse\n",
        "\n",
        "\n",
        "print(\"Training XGBoost model with Time-Series CV for OwnerOccupier properties...\")\n",
        "xgb_owner_final, y_pred_owner, rmse_owner = train_xgboost_with_optuna(X_owner, y_owner, \"OwnerOccupier\")\n",
        "\n",
        "print(\"Training XGBoost model with Time-Series CV for Investment properties...\")\n",
        "xgb_investment_final, y_pred_investment, rmse_investment = train_xgboost_with_optuna(x_investment, y_investment, \"Investment\")\n",
        "\n",
        "print(f\"Final RMSE Results:\")\n",
        "print(f\"RMSE for OwnerOccupier: {rmse_owner:.4f}\")\n",
        "print(f\"RMSE for Investment: {rmse_investment:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "h6qJNQA6cdLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Retrieve timestamps correctly\n",
        "owner_timestamps = train_original.loc[X_owner.index.intersection(train_original.index), \"timestamp\"]\n",
        "investment_timestamps = train_original.loc[X_investment.index.intersection(train_original.index), \"timestamp\"]\n",
        "\n",
        "# Compute Residuals\n",
        "residuals_owner = y_owner - y_pred_owner\n",
        "residuals_investment = y_investment - y_pred_investment\n",
        "\n",
        "# Create DataFrames for Visualization\n",
        "owner_residuals_df = pd.DataFrame({\"timestamp\": owner_timestamps, \"residuals\": residuals_owner})\n",
        "investment_residuals_df = pd.DataFrame({\"timestamp\": investment_timestamps, \"residuals\": residuals_investment})\n",
        "\n",
        "# Sort by timestamp to maintain chronological order\n",
        "owner_residuals_df = owner_residuals_df.sort_values(by=\"timestamp\")\n",
        "investment_residuals_df = investment_residuals_df.sort_values(by=\"timestamp\")\n",
        "\n",
        "# Plot residuals over time for OwnerOccupier properties\n",
        "plt.figure(figsize=(12, 5))\n",
        "sns.scatterplot(x=owner_residuals_df[\"timestamp\"], y=owner_residuals_df[\"residuals\"], alpha=0.5, color=\"blue\")\n",
        "plt.axhline(y=0, color=\"red\", linestyle=\"--\")\n",
        "plt.title(\"Residuals Over Time - OwnerOccupier Properties\")\n",
        "plt.xlabel(\"Timestamp\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Plot residuals over time for Investment properties\n",
        "plt.figure(figsize=(12, 5))\n",
        "sns.scatterplot(x=investment_residuals_df[\"timestamp\"], y=investment_residuals_df[\"residuals\"], alpha=0.5, color=\"orange\")\n",
        "plt.axhline(y=0, color=\"red\", linestyle=\"--\")\n",
        "plt.title(\"Residuals Over Time - Investment Properties\")\n",
        "plt.xlabel(\"Timestamp\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GBmec_vWjcqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the 'id' column is correctly assigned before dropping it for prediction\n",
        "X_test_prepared_df[\"id\"] = test.loc[X_test_prepared_df.index, \"id\"]\n",
        "\n",
        "# Drop 'id' column before making predictions to avoid feature mismatch\n",
        "test_features_owner = X_test_prepared_df[X_test_prepared_df['product_type_OwnerOccupier'] == 1].drop(columns=['id', 'log_price_doc', 'price_doc'], errors='ignore')\n",
        "test_features_investment = X_test_prepared_df[X_test_prepared_df['product_type_Investment'] == 1].drop(columns=['id', 'log_price_doc', 'price_doc'], errors='ignore')\n",
        "\n",
        "# Generate Predictions on Test Data\n",
        "y_test_pred_owner = xgb_owner_final.predict(test_features_owner)\n",
        "y_test_pred_investment = xgb_investment_final.predict(test_features_investment)\n",
        "\n",
        "# Assign predictions back to the test dataset\n",
        "X_test_prepared_df.loc[X_test_prepared_df['product_type_OwnerOccupier'] == 1, 'log_price_doc'] = y_test_pred_owner\n",
        "X_test_prepared_df.loc[X_test_prepared_df['product_type_Investment'] == 1, 'log_price_doc'] = y_test_pred_investment\n",
        "\n",
        "# Convert log predictions back to original scale\n",
        "X_test_prepared_df['price_doc'] = np.expm1(X_test_prepared_df['log_price_doc'])\n",
        "\n",
        "# Ensure test dataset is sorted by 'id'\n",
        "submission = X_test_prepared_df[['id', 'price_doc']].sort_values(by='id')\n",
        "\n",
        "# Save the submission file\n",
        "submission.to_csv('submission3.csv', index=False)\n",
        "\n",
        "print(\"Predictions completed and saved in original order!\")\n"
      ],
      "metadata": {
        "id": "wYsHeLnnX-LR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_file = \"submission3.csv\"\n",
        "submission.to_csv(submission_file, index=False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(submission_file)\n",
        "\n",
        "print(f\"Predictions saved successfully: {submission_file}\")"
      ],
      "metadata": {
        "id": "wf1tEP-Gdfem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute residuals for OwnerOccupier model\n",
        "residuals_owner = y_owner - xgb_owner_final.predict(X_owner)\n",
        "\n",
        "# Compute residuals for Investment model\n",
        "residuals_investment = y_investment - xgb_investment_final.predict(x_investment)"
      ],
      "metadata": {
        "id": "gLu6Oisxh06R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "sns.histplot(residuals_owner, bins=50, kde=True, color=\"blue\", label=\"OwnerOccupier\")\n",
        "sns.histplot(residuals_investment, bins=50, kde=True, color=\"orange\", label=\"Investment\")\n",
        "plt.axvline(0, color=\"red\", linestyle=\"--\")\n",
        "plt.xlabel(\"Residuals\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Histogram of Residuals\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.scatter(xgb_owner_final.predict(x_owner), residuals_owner, alpha=0.5, color=\"blue\", label=\"OwnerOccupier\")\n",
        "plt.scatter(xgb_investment_final.predict(x_investment), residuals_investment, alpha=0.5, color=\"orange\", label=\"Investment\")\n",
        "plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
        "plt.xlabel(\"Predicted Values\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.title(\"Residuals vs. Predicted Values\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oKR4sKjNh7Dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute summary statistics\n",
        "summary_owner = {\n",
        "    \"Mean\": np.mean(residuals_owner),\n",
        "    \"Median\": np.median(residuals_owner),\n",
        "    \"Standard Deviation\": np.std(residuals_owner),\n",
        "    \"Min\": np.min(residuals_owner),\n",
        "    \"Max\": np.max(residuals_owner)\n",
        "}\n",
        "\n",
        "summary_investment = {\n",
        "    \"Mean\": np.mean(residuals_investment),\n",
        "    \"Median\": np.median(residuals_investment),\n",
        "    \"Standard Deviation\": np.std(residuals_investment),\n",
        "    \"Min\": np.min(residuals_investment),\n",
        "    \"Max\": np.max(residuals_investment)\n",
        "}\n",
        "\n",
        "print(\"OwnerOccupier Residual Summary:\", summary_owner)\n",
        "print(\"Investment Residual Summary:\", summary_investment)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9oDfP_4Xh_iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute residuals\n",
        "residuals_owner = y_owner - xgb_owner_final.predict(x_owner)\n",
        "\n",
        "# Compute IQR for outlier detection\n",
        "Q1 = np.percentile(residuals_owner, 25)\n",
        "Q3 = np.percentile(residuals_owner, 75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define outlier thresholds\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Identify outliers\n",
        "outliers_owner = (residuals_owner < lower_bound) | (residuals_owner > upper_bound)\n",
        "\n",
        "# Get the indices of outliers\n",
        "outlier_ids = y_owner.index[outliers_owner]\n",
        "\n",
        "# Remove outliers from both X_owner and y_owner\n",
        "x_owner_cleaned = x_owner.drop(index=outlier_ids)\n",
        "y_owner_cleaned = y_owner.drop(index=outlier_ids)\n",
        "\n",
        "print(f\"Removed {len(outlier_ids)} outliers from OwnerOccupier dataset.\")\n"
      ],
      "metadata": {
        "id": "mrnBo-UqjhAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve best parameters from previous Optuna study\n",
        "best_params_owner = xgb_owner_final.get_params()  # Get the best params from the previous trained model\n",
        "\n",
        "# Train a new model using the cleaned dataset\n",
        "xgb_owner_refit = xgb.XGBRegressor(**best_params_owner)\n",
        "xgb_owner_refit.fit(x_owner_cleaned, y_owner_cleaned)\n",
        "\n",
        "# Make predictions on training data\n",
        "y_pred_owner_refit = xgb_owner_refit.predict(x_owner_cleaned)\n",
        "\n",
        "# Compute new RMSE\n",
        "rmse_owner_refit = np.sqrt(mean_squared_error(y_owner_cleaned, y_pred_owner_refit))\n",
        "\n",
        "print(f\"New RMSE after outlier removal: {rmse_owner_refit:.4f}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QOwzmnD_jfVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure 'id' column is correctly assigned before making predictions\n",
        "X_test_prepared_df[\"id\"] = test.loc[X_test_prepared_df.index, \"id\"]\n",
        "\n",
        "# Drop 'id' and any unnecessary columns to match training features\n",
        "test_features_owner = X_test_prepared_df[X_test_prepared_df['product_type_OwnerOccupier'] == 1].drop(columns=['id', 'log_price_doc', 'price_doc'], errors='ignore')\n",
        "test_features_investment = X_test_prepared_df[X_test_prepared_df['product_type_Investment'] == 1].drop(columns=['id', 'log_price_doc', 'price_doc'], errors='ignore')\n",
        "\n",
        "# Generate Predictions using the trained models\n",
        "y_test_pred_owner = xgb_owner_refit.predict(test_features_owner)\n",
        "y_test_pred_investment = xgb_investment_final.predict(test_features_investment)\n",
        "\n",
        "# Assign predictions back to the test dataset\n",
        "X_test_prepared_df.loc[X_test_prepared_df['product_type_OwnerOccupier'] == 1, 'log_price_doc'] = y_test_pred_owner\n",
        "X_test_prepared_df.loc[X_test_prepared_df['product_type_Investment'] == 1, 'log_price_doc'] = y_test_pred_investment\n",
        "\n",
        "# Convert log predictions back to original scale\n",
        "X_test_prepared_df['price_doc'] = np.expm1(X_test_prepared_df['log_price_doc'])\n",
        "\n",
        "# Merge predictions back into original test dataset to preserve order\n",
        "submission = test[['id']].merge(X_test_prepared_df[['id', 'price_doc']], on='id', how='left')\n",
        "\n",
        "# Save the submission file\n",
        "submission.to_csv('submission_final.csv', index=False)\n",
        "\n",
        "print(\"Final predictions completed and saved in original order!\")\n",
        "\n",
        "\n",
        "# Save the final predictions to a CSV file\n",
        "submission_file = 'submission_final.csv'\n",
        "submission.to_csv(submission_file, index=False)\n",
        "\n",
        "# Provide a download link\n",
        "files.download(submission_file)\n",
        "\n",
        "print(\"Final predictions completed and saved as 'submission_final.csv'!\")"
      ],
      "metadata": {
        "id": "COlPlLb4MRvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure 'id' column is correctly assigned before making predictions\n",
        "X_test_prepared_df[\"id\"] = test.loc[X_test_prepared_df.index, \"id\"]\n",
        "\n",
        "# Drop 'id' and any other unnecessary columns to match training features\n",
        "test_features_owner = X_test_prepared_df[X_test_prepared_df['product_type_OwnerOccupier'] == 1].drop(columns=['id', 'log_price_doc', 'price_doc'], errors='ignore')\n",
        "test_features_investment = X_test_prepared_df[X_test_prepared_df['product_type_Investment'] == 1].drop(columns=['id', 'log_price_doc', 'price_doc'], errors='ignore')\n",
        "\n",
        "# Generate Predictions using the newly trained models\n",
        "y_test_pred_owner = xgb_owner_refit.predict(test_features_owner)\n",
        "y_test_pred_investment = xgb_investment_final.predict(test_features_investment)  # Use the best investment model\n",
        "\n",
        "# Assign predictions back to the test dataset\n",
        "X_test_prepared_df.loc[X_test_prepared_df['product_type_OwnerOccupier'] == 1, 'log_price_doc'] = y_test_pred_owner\n",
        "X_test_prepared_df.loc[X_test_prepared_df['product_type_Investment'] == 1, 'log_price_doc'] = y_test_pred_investment\n",
        "\n",
        "# Convert log predictions back to original scale\n",
        "X_test_prepared_df['price_doc'] = np.expm1(X_test_prepared_df['log_price_doc'])\n",
        "\n",
        "# Ensure test dataset is sorted by 'id'\n",
        "submission = X_test_prepared_df[['id', 'price_doc']].sort_values(by='id')\n"
      ],
      "metadata": {
        "id": "09HvOWE9kTZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the final predictions to a CSV file\n",
        "submission_file = 'submission_final.csv'\n",
        "submission.to_csv(submission_file, index=False)\n",
        "\n",
        "# Provide a download link\n",
        "files.download(submission_file)\n",
        "\n",
        "print(\"Final predictions completed and saved as 'submission_final.csv'!\")\n"
      ],
      "metadata": {
        "id": "uviEoBrOkUM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GUBvgJBmMLsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WkG4i_e4MLv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AOjhjFIyMLx7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}